# build_rag_vectorstore_from_dir.py

import os
import re
import sys
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from pathlib import Path

# Add the parent directory to Python path so we can import from agents
current_dir = Path(__file__).parent.absolute()
backend_dir = current_dir.parent.parent
sys.path.append(str(backend_dir))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

os.environ["CHROMA_TELEMETRY_ENABLED"] = "false"

from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from llm_loader import get_embedding_model

@dataclass
class LegalMetadata:
    """Metadata for legal document chunks"""
    filename: str
    page_number: int
    chapter: str = ""
    section: str = ""
    referenced_sections: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {k: v for k, v in self.__dict__.items() if v}

class LegalDocumentSplitter:
    """Structure-aware splitter for legal documents"""
    
    def __init__(self):
        self.patterns = {
            'chapter': r'CHAPTER [A-Z]+',
            'section': r'(?:SECTION|SEC\.) \d+(?:\([a-z\d]+\))?',
            'definition': r'(?:^|\n)"[^"]+"\s+means',
            'numbered_clause': r'(?:^|\n)\d+\.\s+',
            'roman_numeral': r'(?:^|\n)[ivxIVX]+\.\s+',
            'cross_reference': r'(?:Section|Sec\.) \d+(?:\([a-z\d]+\))?'
        }
        
        self.dense_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", ". ", " "]
        )
        
        self.narrative_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            length_function=len,
            separators=["\n\n", "\n", ". ", " "]
        )

    def extract_metadata(self, text: str, filename: str, page_number: int) -> LegalMetadata:
        chapter_match = re.search(self.patterns['chapter'], text)
        section_match = re.search(self.patterns['section'], text)
        references = list(set(re.findall(self.patterns['cross_reference'], text)))
        
        return LegalMetadata(
            filename=filename,
            page_number=page_number,
            chapter=chapter_match.group(0) if chapter_match else "",
            section=section_match.group(0) if section_match else "",
            referenced_sections=references
        )
    
    def is_dense_content(self, text: str) -> bool:
        definition_pattern = self.patterns['definition']
        numbered_pattern = self.patterns['numbered_clause']
        subsection_pattern = r'\([a-z\d]+\)'
        
        return bool(
            re.search(definition_pattern, text) or
            len(re.findall(numbered_pattern, text)) > 2 or
            len(re.findall(subsection_pattern, text)) > 2
        )
    
    def split_document(self, text: str, filename: str, page_number: int) -> List[Document]:
        chunks = []
        
        # Find section boundaries
        boundaries = []
        for pattern_name, pattern in self.patterns.items():
            if pattern_name != 'cross_reference':
                for match in re.finditer(pattern, text):
                    boundaries.append(match.start())
        
        # Sort and add end of text
        boundaries = sorted(list(set(boundaries)))
        if not boundaries or boundaries[-1] != len(text):
            boundaries.append(len(text))
        
        # Process each section
        start_idx = 0
        for end_idx in boundaries:
            if end_idx <= start_idx:
                continue
                
            section_text = text[start_idx:end_idx].strip()
            if not section_text:
                start_idx = end_idx
                continue
            
            # Get metadata for the section
            metadata = self.extract_metadata(section_text, filename, page_number)
            
            # Choose splitter based on content type
            splitter = self.dense_splitter if self.is_dense_content(section_text) else self.narrative_splitter
            
            # Split into chunks
            for chunk in splitter.split_text(section_text):
                chunks.append(Document(
                    page_content=chunk,
                    metadata=metadata.to_dict()
                ))
            
            start_idx = end_idx
        
        return chunks

def process_pdfs(directory_path: str) -> List[Document]:
    """Process PDFs and split into chunks"""
    logging.info(f"Processing PDFs from directory: {directory_path}")
    documents = []
    splitter = LegalDocumentSplitter()
    
    for filename in os.listdir(directory_path):
        if not filename.lower().endswith('.pdf'):
            continue
            
        pdf_path = os.path.join(directory_path, filename)
        logging.info(f"Processing file: {pdf_path}")
        
        try:
            reader = PdfReader(pdf_path)
            for page_num, page in enumerate(reader.pages, 1):
                logging.info(f"Processing page {page_num}")
                text = page.extract_text()
                
                if text:
                    preview = text[:100].replace('\n', ' ')
                    logging.info(f"Page {page_num} content preview: {preview}...")
                    
                    chunks = splitter.split_document(text, filename, page_num)
                    documents.extend(chunks)
                    
                    if chunks:
                        logging.info(f"Created {len(chunks)} chunks from page {page_num}")
                        logging.info(f"Sample chunk metadata: {chunks[0].metadata}")
            
            logging.info(f"‚úÖ Successfully processed {filename}")
            
        except Exception as e:
            logging.error(f"‚ùå Failed to process {filename}: {str(e)}")
            logging.exception("Full traceback:")
    
    logging.info(f"Total documents processed: {len(documents)}")
    return documents

def create_vectorstore_from_directory(pdf_dir: str, persist_directory: str = "./rag_db"):
    """Create and persist vector store from documents"""
    print("üìÇ Reading and processing PDF files...")
    documents = process_pdfs(pdf_dir)
    
    if not documents:
        print("‚ùå No valid documents processed.")
        return
    
    # Print statistics
    total_chunks = len(documents)
    avg_chunk_size = sum(len(doc.page_content) for doc in documents) / total_chunks
    print(f"\nüìä Processing Statistics:")
    print(f"   Total chunks: {total_chunks}")
    print(f"   Average chunk size: {avg_chunk_size:.0f} characters")
    
    if documents:
        print("\nüìë Sample chunk metadata:")
        for key, value in documents[0].metadata.items():
            print(f"   {key}: {value}")
    
    print("\nüìå Embedding and storing in Chroma DB...")
    embedding = get_embedding_model()
    
    vectordb = Chroma.from_documents(
        documents=documents,
        embedding=embedding,
        persist_directory=persist_directory
    )
    vectordb.persist()
    print(f"‚úÖ Vector DB created at {persist_directory}")

if __name__ == "__main__":
    try:
        # Get current directory and set up paths
        current_dir = Path(__file__).parent.absolute()
        pdf_path = current_dir / "CP Act 2019_1732700731.pdf"
        db_path = current_dir / "rag_db"
        
        print(f"PDF path: {pdf_path}")
        print(f"DB path: {db_path}")
        
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF file not found at {pdf_path}")
        
        # Ensure DB directory exists
        db_path.mkdir(parents=True, exist_ok=True)
        
        # Create vectorstore
        create_vectorstore_from_directory(
            str(current_dir),
            persist_directory=str(db_path)
        )
    except Exception as e:
        print(f"Error building vectorstore: {str(e)}")
        import traceback
        traceback.print_exc()
